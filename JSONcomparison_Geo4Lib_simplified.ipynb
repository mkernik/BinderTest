{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration, we'll look at a script that: \n",
    "1) steps through a list of geospatial data portals\n",
    "2) takes a snapshot of DCAT metadata for the items available in each portal, and \n",
    "3) compares it to a previous snapshot of DCAT metadata from the same portals to see what has changed\n",
    "\n",
    "The Big Ten Academic Alliance Geoportal project uses a more complex version of this script to keep track of changes to the publicly available data that are included in its database.  Open geospatial data can be remarkably unstable.  Data providers often seek to have the most up-to-date version of layers, meaning that historical spatial data may not continue to be hosted.  To keep our discovery tool from returning lots of broken results and to capture newly added resources, we frequently re-check data portals. \n",
    "\n",
    "There are several different platform technology on which open geospatial data can be found. These platforms have slightly different metadata schemas and different levels of consistency in how each element is formatted.  For this script, we will look at ArcGIS Open Data portals.\n",
    "\n",
    "As always, we need to start by importing the modules we'll need. Everything here is part of Python's standard library, so you shouldn't need to install any additional modules to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will manually set a few items based on the date the report is being run, the date of the previous snapshot we want to compare to, and the location of the file directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set the date download of the older and newer jsons\n",
    "PreviousActionDate = '20171106'\n",
    "ActionDate = '20191231'\n",
    "\n",
    "#creates a location to put the results of this script\n",
    "if not os.path.exists('C:/data_harvesting_Geo4Lib2020'):\n",
    "    os.makedirs('C:/data_harvesting_Geo4Lib2020')\n",
    "\n",
    "## name of the main folders containing input files and output locations\n",
    "in_directory = 'data/'\n",
    "out_directory = 'C:/data_harvesting_Geo4Lib2020/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we set up a loop to check through multiple portals at once, let's look at how the comparison will work with a single portal.  \n",
    "\n",
    "First we need to tell the script the portal name (in this case a code used by the project to identify each portal) and URL endpoint for the portal's metadata in JSON format.  We will also tell the script where to find old snapshots of this metadata and where to put a copy of the new snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "portalName = \"05b-053\" \n",
    "url = \"http://gis-hennepin.opendata.arcgis.com/data.json\"\n",
    "\n",
    "oldjson = in_directory + '%s_%s.json' % (portalName, PreviousActionDate)\n",
    "newjson = out_directory + '%s_%s.json' % (portalName, ActionDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will open the webpage, load the metadata as a variable that can be immediately used by the script, and save a snapshot copy of the metadata to be used as a comparison file in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = urllib.request.urlopen(url)\n",
    "newdata = json.load(response)\n",
    "\n",
    "with open(newjson, 'w') as outfile:  \n",
    "    json.dump(newdata, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also check to see whether there is a snapshot of the metadata from the previous date that we can use to see how things have changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(oldjson):\n",
    "    with open(oldjson) as data_file:    \n",
    "        olderdata = json.load(data_file)\n",
    "    print (\"Comparison file found!\")\n",
    "else:\n",
    "    print (\"There is no comparison file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you open the metadata URL in a browser (and have a human-friendly json viewer plugin in your browser), you can look at the structure of the metadata available for each portal and resource.  \n",
    "\n",
    "There are a few top-level elements including \"dataset:\" which is a list of dictionaries.  Each dictionary represents one data layer with unique metadata element (key) / metadata (value) information. \n",
    "\n",
    "{\n",
    "@type: \"dcat:Dataset\",\n",
    "identifier: \"http://gis-hennepin.opendata.arcgis.com/datasets/c5f67030adce4b3987782a88e0652093_9\",\n",
    "title: \"US Congressional Districts\",\n",
    "description: \"Hennepin County US Congressional Districts based on the 2012 redistricting of US Congressional district polygons determined from Legislative district boundaries.\",\n",
    "+keyword: [],\n",
    "issued: \"2015-09-14T18:04:17.000Z\",\n",
    "modified: \"2019-01-11T22:22:45.610Z\",\n",
    "+publisher: {},\n",
    "+contactPoint: {},\n",
    "accessLevel: \"public\",\n",
    "+distribution: [],\n",
    "landingPage: \"http://gis-hennepin.opendata.arcgis.com/datasets/c5f67030adce4b3987782a88e0652093_9\",\n",
    "webService: \"https://gis.hennepin.us/arcgis/rest/services/HennepinData/BOUNDARIES/MapServer/9\",\n",
    "license: \"https://hub.arcgis.com/api/v2/datasets/c5f67030adce4b3987782a88e0652093_9/license\",\n",
    "spatial: \"-93.7728,44.783,-93.1766,45.2471\",\n",
    "+theme: [],\n",
    "},\n",
    "\n",
    "We are interested in printing a report that only includes information about the items that have been newly added since the last time the script was run (or in this case Nov 6th, 2017).  To do this, we first need to make a list of the identifiers for all of the items that were in the older metadata snapshot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "older_ids = {}\n",
    "for x in range(len(olderdata[\"dataset\"])):\n",
    "    older_ids[x] = olderdata[\"dataset\"][x][\"identifier\"]\n",
    "    \n",
    "print (len(older_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet is the heart of the comparison. \n",
    "We compare the identifier of each item found in the newly downloaded metadata against the list of identifiers from the older metadata snapshot.  If it an item is NOT there, the script adds the values for them to a dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newItemDict = {}\n",
    "\n",
    "for y in range(len(newdata[\"dataset\"])):\n",
    "    identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "    if identifier not in older_ids.values():\n",
    "        metadata = []\n",
    "        metadata.append(portalName)\n",
    "        metadata.append(identifier)\n",
    "        metadata.append(newdata[\"dataset\"][y]['title'])\n",
    "        metadata.append(newdata[\"dataset\"][y]['description'])\n",
    "        metadata.append(newdata[\"dataset\"][y]['issued'])\n",
    "        metadata.append(newdata[\"dataset\"][y]['landingPage'])\n",
    "        newItemDict[identifier] = metadata\n",
    "\n",
    "print(len(newItemDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dictionary with information about each of the new items we can print it to a CSV report!\n",
    "\n",
    "NewItemDict is a dictionary with item identifiers as keys and a list of metadata as values.\n",
    "\n",
    "{\n",
    "identifier1: [portal name, identifier, title, description, issued date, landing page],\n",
    "identifier2: [portal name, identifier, title, description, issued date, landing page],\n",
    "identifier3: [portal name, identifier, title, description, issued date, landing page],\n",
    "...\n",
    "}\n",
    "\n",
    "First we need to set the field names and the order they should be printed to the spreadsheet (matching the order of metadata elements we collected for each item).  Also the path where the CSV report should be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [\"portalName\", \"identifier\", \"title\", \"description\", \"issued_date\", \"landingPage\"]\n",
    "\n",
    "report = out_directory + \"NewItems_%s.csv\" %  (ActionDate)\n",
    "\n",
    "with open(report, 'w', newline='') as outfile:\n",
    "    csvout = csv.writer(outfile)\n",
    "    csvout.writerow(fields)\n",
    "    for keys in newItemDict:\n",
    "        allvalues = newItemDict[keys]\n",
    "        csvout.writerow(allvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your directory for the new file!\n",
    "\n",
    "The next code block shows what it would look like to run this script to check the contents of multiple portals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newItemDict = {}\n",
    "\n",
    "with open(in_directory + 'PortalList.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        portalName = row['portalName']\n",
    "        url = row['URL']\n",
    "        print (portalName, url)\n",
    "        \n",
    "        oldjson = in_directory + '%s_%s.json' % (portalName, PreviousActionDate)\n",
    "        newjson = out_directory + '%s_%s.json' % (portalName, ActionDate)\n",
    "        \n",
    "        response = urllib.request.urlopen(url)\n",
    "        newdata = json.load(response)\n",
    "\n",
    "        with open(newjson, 'w') as outfile:  \n",
    "            json.dump(newdata, outfile)\n",
    "            \n",
    "        if os.path.exists(oldjson):\n",
    "            with open(oldjson) as data_file:    \n",
    "                olderdata = json.load(data_file)\n",
    "        else:\n",
    "            print (\"There is no comparison file!\")\n",
    "            \n",
    "        older_ids = {}\n",
    "        for x in range(len(olderdata[\"dataset\"])):\n",
    "            older_ids[x] = olderdata[\"dataset\"][x][\"identifier\"]\n",
    "\n",
    "        for y in range(len(newdata[\"dataset\"])):\n",
    "            identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "            if identifier not in older_ids.values():\n",
    "                metadata = []\n",
    "                metadata.append(portalName)\n",
    "                metadata.append(identifier)\n",
    "                metadata.append(newdata[\"dataset\"][y]['title'])\n",
    "                metadata.append(newdata[\"dataset\"][y]['description'])\n",
    "                metadata.append(newdata[\"dataset\"][y]['issued'])\n",
    "                metadata.append(newdata[\"dataset\"][y]['landingPage'])\n",
    "                newItemDict[identifier] = metadata\n",
    "\n",
    "with open(report, 'w', newline='') as outfile:\n",
    "    csvout = csv.writer(outfile)\n",
    "    csvout.writerow(fields)\n",
    "    for keys in newItemDict:\n",
    "        allvalues = newItemDict[keys]\n",
    "        csvout.writerow(allvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "\n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(15, 4)), columns=list('ABCD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a download=\"data.csv\" href=\"data:text/csv;base64,QSxCLEMsRAozNSwxNiw0NiwzOQozNCw0Niw0LDU0Cjc0LDMsMzIsNzkKNiwyMCwzNCw5MgoxNywyLDc2LDY4Cjk5LDExLDYxLDEyCjMwLDEyLDM1LDkKMzcsNCw2MCw0OAo3NSw1MSw2Myw0Ngo2OSw5OCw4MCw1Nwo2Miw5NCw1NSwzNAoyOSw1MSw1NywxMgo4MCw2OCw2NSwzCjIwLDg1LDQ2LDc4Cjc3LDEyLDgzLDkyCg==\" target=\"_blank\">Download CSV file</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import base64  \n",
    "import pandas as pd  \n",
    "\n",
    "def create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    csv = df.to_csv(index =False)\n",
    "    b64 = base64.b64encode(csv.encode())\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "create_download_link(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more complicated version of this script:\n",
    "1) collects additional metadata elements for each item and adds blank spaces for elements that cannot be automated\n",
    "2) creates reports about deleted items and the overall number of records/changes to portals\n",
    "3) cleans html tags and other formatting issues out of the description\n",
    "4) has more error catching and reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
